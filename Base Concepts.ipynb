{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'DL_Basic_Architecture.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neural network is made up of individual nodes which are called perceptron. A neural network contains i/p layer (features), hidden layer and o/p(target) layer as seen above. Training happens with a combination of forward and backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Neural Network works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'How_NN_works.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each node of a neural network in a forward propagation, two operations happen. One is the summation of weights*inputs + bias. Second is the activation function, like sigmoid. These activation functions help to determine if a node(perceptron/neuron) should be activated or not. Similar to how if a hot object is placed in our one hand, the neurons there get activated by the stimulus and transfer data to the brain. But the other hand will not be activated as there is no heat stimulus there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and ReLu Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Sigmoid_Relu_AF.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid activation function converts the value given to it between 0 and 1. The value could be in any range but sigmoid will turn in between 0 and 1. Relu activation funtion returns 0 if the value passed is negative. If +ve, it gives the same +ve value as result (as seen above). So when to use what? The hidden layers can use whatever functions depending on the problem statement and architecture. For classification use cases, the output layer is to use sigmoid AF. For regression use cases, the output layer is to use ReLu AF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Training_NN_with_backpropagation.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of a forward propagation, the output yhat is compared with the actual y, and the loss is calculated using the loss function (seen above). Now since the output is different, the weights has to be modified in neurons to match the actual output. This is done in backpropagation, using an optimizer. Some examples for optimizers are gradient descent, stochastic gradient descent, adam, etc., The weights in the nodes are modified with the formula seen above (written in red marker). The combination of one forward and backward propagation is called an epoch. The epochs continue till we have reduced the loss till the global minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Multi Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Multilayer_NN_training.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multilayer neural network, similar workflow happens like a single layer neural network. But there will be multiple weights at each layer, so we can see respective weight matrices for each layer. To determine when to stop and know we have desired output, gradient descent optimizer is used. In a gradient descent, in the new weight formula we need to always use a small learning rate so that the convergence can happen. A big value will jump across and will not get to the global minima whereas a too small learning rate can take forever to reach the minima. Also in gradient descent, -ve slope moves the weights towards the right whereas a +ve slope moves the weights towards the left (towards global minima). In weight updation one weight can affect the weights of multiple nodes. So, this is done using Chain rule, which is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule of differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Chain_rule_1.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Chain_rule_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first image above, the output node is directly affected by the node f21. So the differentiation takes only one path and is simple for the last layer. But look at the second layer. For w11(2), the output node f31 is affected by taking two paths. So both those paths has to be included in the differentiation, and since weight from one layer affects the next layer till the output node, the weights from the output node till the node where we are updating the weights must be taken into consideration. Similarly, the chain rule will apply for all layers. So must be careful to know that all the paths affecting the output must be taken into the differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Vanishing_gradient.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem happens with using sigmoid activation function in hidden layers of large multi layer neural networks. Remember, sigmoid activation function gives an output value between 0 and 1 always. The derivative of the sigmoid activation function will always range between 0 and 0.25. Consider the example shown above. The derivative of O21 and O11 both will be between 0 and 0.25. Multiplying them will again result in a smaller value. This is just two derivatives. Imagine a deep neural network with multiple hidden layers and nodes. There those derivatives ranging between 0 and 0.25 will get multiplied and will result in a very small value. And after that, they are again multiplied with a small learning rate value. So the output value will be very less and the updated weight will be almost similar to the previous weight, which gives no use to the model at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Exploding_gradient.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploding gradient problem happens because of the weights. It does not necessarily depend on what activation function is used. Consider the example above. In calculating w11(1) new, the chain rule is applied and we have the derivatives for the backpropagation path. Consider the derivative of O21 in the chain rule which is boxed. It can be replaced as a derivative of sigmoid activation of Z as shown above (similar chain rule is applied). The first term is derivative of sigmoid function, which gives value between 0 and 0.25 which is less. The second, derivative of Z (Z = w21 * O11 + b2) with respect to O11 will become w21 due to differentiation.  If that weight is large, it will result in a large number. This is just in one derivative in the chain rule. Imagine in a multi layer neural network with large weights. The chain rule will be a multiplication of large numbers and hence the resulting updated weight will be very very varying with the old weight. Due to which, the convergence at the global minima will never occur. Hence, it is necessary to assign correct weights to the nodes in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop out Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Drop_Out.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout in Neural Network is used to avoid overfitting condition in a deep neural network. The workflow is similar to random forest where in each tree subset of features is selected to construct a tree. In dropout, at each iteration some inputs and some neurons in each layer will randomly be turned off and the epoch proceeds on that to do the training. At the end while predicting, to compensate for the dropped out neurons, the weights are multiplied with the dropout ratio (W*p). The dropout ratio is between 0 and 1. The common value for p in a deep neural network should be around or above 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'ReLU_1.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function gives the output between 0 and 1. The derivative of sigmoid is between 0 and 0.25. The threshold activation function tanh gives output between -1 and 1 and it's derivative is between 0 and 1. These lead to the vanishing gradient problem where the derivatives of these are always less than 1. So ReLU is used to overcome that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'ReLU_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU is a simple fuction of max(0,z). So gives only positive output. The deriviative is 1 when z > 0 and 0 if z < 0. This will solve as value output will be mostly 1, but in rare cases where the derivative is 0, in backpropagation the neuron will be dead. So to overcome that, leaky relu is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Leaky_Relu.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In leaky relu, a small value is added to the negative portion in order to make sure that the derivate is not zero. It's derivatives are as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Weight_Init_1.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights should be small but not too very small coz it will cause vanishing problem. The weights should not be the same, coz if it is then each neuron will learn in the same way from the inputs and the learning will not be proper as they all will behave the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Weight_Init_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Weight_Init_3.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These both work well with sigmoid activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Weight_Init_4.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well with ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'SGD_1.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is used to reduce the cost of computation power. When all the data points are used at the same time in weights updation (backpropagation) it is gradient descent. When 1 data point is used at a time, it is SGD. When K (batch size) data points is used it is mini batch SGD. Mini batch SGD is used to do the computations in a less costly way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'SGD_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gradient descent the convergence will happen directly and fastly at global minima coz it uses all data points. But in mini batch SGD, it will take a zig zag path to convergence as seen above, which creates some noisy data points. And the weights updated (derivative of loss with respect to weights) will only be approximately similar to the update on gradient descent and will never be the exact same as it takes only a batch of data points for updation at each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minima and Maxima (Local and global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Minima_Maxima.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different curves come for different loss functions. The curve on the right in the above image is from MSE. There in the regular curve there is one global minima, but there are no maximas present. In the ulta one, there is one global maxima but no minimas present. It can extend to any value. In the curve to the left, there are multiple local minimas and maximas but there is a global minima and maxima, which should be attained instead of local ones. But on each of the minima and the maxima the slope will be 0 and the weight updation will not happen. There are points like saddle points as seen above which will also be present to confuse us. They will also have 0 slope. To overcome this problem, momentum is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Convex_Functions.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convex functions we can see in machine learning regression models, which will have only one minima which is global minima. It is convex because two points inside the curve at any place can be joined with a line which falls within the same curve. Non convex functions are common in deep learning problems where the line joining two points will go through differect curve areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Momentum.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept behind momentum is that the highest weightage is given to the data point which is in consideration at that point in time. Momentum smooths out the jumps in SGD and reduces the noise to some extent, but not fully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Momentum_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum formula is shown above and the common value for gamma is 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Gradient (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Adagrad.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad is developed with the idea of a goal that for each neuron in each hidden layer for each iteration to have different learning rates so that the network can learn much more deeply about the data points and features. The learning rates updation formula is shown above. The epsilon (small +ve number) value in the denominator is to avoid the divide by 0 errors. As the iteration goes on (i value increases) the alpha t value in denominator increases and the learning rate decreases. So the jumps towards global minima will be big at first and as time and iteration goes on it decreases gradually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Adagrad_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of adagrad is that as the iterations increase the alpha t value will increase like anything and the learning rate will become so small that virtually the updated weight is the same as the old weight. To overcome this problem, adadelta is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Adadelta.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta overcomes the dramatic decrease in the learning rate value by restricting the denominator in the learning rate updation formula by replacing alpha t with weighted average, for which the formula is given above. With weighted average also the learning rate value will decrease but it will not decrease as much as it did for adagrad updation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
